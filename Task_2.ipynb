{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing PDF file\n",
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name: Aniruddha Indurkar\n",
    "#### Date: 14/04/2019\n",
    "\n",
    "Environment: Python 3.6.3 and Jupyter notebook Libraries used\n",
    "\n",
    "* PyPDF2\n",
    "* re\n",
    "* nltk.collocation\n",
    "* nltk.tokenise\n",
    "* matplotlib\n",
    "* itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "This analysis extracts data from an pdf-based corpus containing 200 unit information. Data was extracted by reading the input file 'Data.pdf', splitting at the document by recognising patterns and then creating a list where each item in the list holds the full XML for each individual patent.\n",
    "\n",
    "Data relating to outcome and synopsis was extracted using the package `PyPDF2` to extract values from specific pdf document. Similarly, the text associated with units for further pre-processing.\n",
    "\n",
    "Text pre-processing was performed with the objective of extracting and transforming the information for each unit into a vector space model.  The pre-processing included tokenisation (unigrams and bigrams), stemming and removal of stopwords. Additionally, the most frequent and least frequent words were removed, and meaningful bigrams were identified.  The initial tokenised vocabulary of the corpus was 9690 words, which was reduced to 4906 words following pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make use of PyPDF2 to read the files\n",
    "import PyPDF2\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening the pdf and saving the file in variable\n",
    "\n",
    "#### Reference of the code:\n",
    "* Site: Stackoverflow\n",
    "* Webpage title: How to read line by line in pdf file using PyPdf?\n",
    "* url: https://stackoverflow.com/questions/2481945/how-to-read-line-by-line-in-pdf-file-using-pypdf\n",
    "* Author: Sandeep\n",
    "* Date: 05/05/2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the pdf file\n",
    "# The following code has been referred from stackoverflow from the site given below:\n",
    "#https://stackoverflow.com/questions/2481945/how-to-read-line-by-line-in-pdf-file-using-pypdf\n",
    "\n",
    "#author: Sandeep (May 5, 2011)\n",
    "pdf_file = open('29389429.pdf', 'rb')\n",
    "\n",
    "read_pdf = PyPDF2.PdfFileReader(pdf_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code has been referred from stackoverflow from the site given below:\n",
    "#https://stackoverflow.com/questions/2481945/how-to-read-line-by-line-in-pdf-file-using-pypdf\n",
    "\n",
    "#author: Sandeep (May 5, 2011)\n",
    "\n",
    "# We store the number of pages\n",
    "number_of_pages = read_pdf.getNumPages()\n",
    "page = read_pdf.getPage(1)\n",
    "page_content = page.extractText()\n",
    "data=page_content.encode('utf-8')\n",
    "\n",
    "# Converting the data into string\n",
    "data=str(data)\n",
    "\n",
    "# Observe the file pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We observe a pattern that the information for a particular unit including the synopsis and outcome is seperated by a ']'. Thus, we exploit this observation and split the data for a units accordingly. \n",
    "\n",
    "For example,\n",
    "\n",
    "b\"CPS5002\\nThis unit will explore the history, theory and principles\\nof Developmental Observation and its relevance to\\nclinical and consultative work with children and\\nadolescents. There will be a consideration of the\\nnature of the links between observation and the\\ndevelopmental theories. Students will be able to\\nexplore these areas both through the context of the\\ncourse work and prescribed observations of children\\nin their naturalistic settings.\\n ['evaluate the relevance of observation skills to the\\nunderstanding of children and adolescents in both\\ntherapeutic and consultative work;', 'analyse the\\nnature of links between observation and the\\ndevelopmental theories;', 'monitor and evaluate their\\nown responses to what is observed and critically apply\\nthese in their work;', 'critically evaluate their\\nobservations in discussion and in written assignments;\\nand', 'integrate Developmental Observation theory,\\nhistory and principles with observation experiences\\nand their clinical work, demonstrating these processes\\nin accounts of their own learning.'].\n",
    "\n",
    "### Similarly, the key distinguisher between outcome and synopsis is '['.\n",
    "### We make use of regex to extract the unit code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the unit from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_code(string):\n",
    "    #The pattern we find in the pdf document. Split according to the '['\n",
    "    a1=string.split(\"[\")\n",
    "    \n",
    "    # Slice for unit information with synopsis\n",
    "    # we expect the unit code to be within the first 10 characters.\n",
    "    a2=a1[0][:10]\n",
    "\n",
    "    #return a valid unit code.\n",
    "    if len(re.findall(\"\\w{3}\\d{4}\",str(a2)))!=0:\n",
    "        \n",
    "        return re.findall(\"\\w{3}\\d{4}\",str(a2))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Synopsis(data):\n",
    "    a1=data.split(\"[\")\n",
    "    \n",
    "    return re.sub(\"\\\\\\\\n\",\" \",a1[0][11:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the outcome\n",
    "\n",
    "The list of outcomes ends with \";\" . In this case we replace these by \".\", in order to create stop words, so that the normalisation process will be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Outcome(data):\n",
    "    # Split to get the string we want\n",
    "    a1=data.split(\"[\")\n",
    "    \n",
    "    # removing unwanted tokens\n",
    "    out=re.sub(\"\\\\\\\\n\",\" \",\"[\"+a1[1]+\"]\")\n",
    "    out=re.sub(\"\\\\\\\\\",\"\",out)\n",
    "    \n",
    "    # Subbing the ; with . \";\" represents the end of sentence in token. We do that in order to normalise correctly\n",
    "    out=re.sub(\";\",\".\",out)\n",
    "    \n",
    "    b1=re.findall(\"\\[(.*?)]\",out)\n",
    "    \n",
    "    # Cleaning the string\n",
    "    out=re.sub(\"', '\",\" \",out)\n",
    "    out=re.sub(\"\\[\",\"\",out) \n",
    "    \n",
    "    return re.sub(\"]\",\"\",out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the entire data to extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_pages = read_pdf.getNumPages()\n",
    "\n",
    "# List to store information about each unit\n",
    "a=list()\n",
    "\n",
    "# Looping through the number of pages\n",
    "for i in range(number_of_pages):\n",
    "    \n",
    "    page = read_pdf.getPage(i)\n",
    "    page_content = page.extractText()\n",
    "    data=page_content.encode('utf-8')\n",
    "    data=str(data)\n",
    "    \n",
    "    # Substituting characters that are not required\n",
    "    re.sub(\"\\\\\\\\n\",\" \",data)\n",
    "    re.sub(\"\\\\\\\\\",\"\",data)\n",
    "    \n",
    "    # Title Synopsis and outcome are occuring on the start of every page hence they are removed\n",
    "    data=re.sub(\"Title\\\\\\\\nSynopsis\\\\\\\\nOutcomes\\\\\\\\n\",\"\",data)\n",
    "    k=data.split(\"]\")    \n",
    "\n",
    "   \n",
    "    for i in k:\n",
    "        # Dictionary containing synopsis and outcome\n",
    "        ad=dict()\n",
    "\n",
    "        units=unit_code(i)\n",
    "\n",
    "        if units is not None:\n",
    "            ad['Unit']=units  \n",
    "            ad['Outcome']=Outcome(i)\n",
    "            ad['Synopsis']=Synopsis(i)\n",
    "            a.append(ad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refining the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refining the dictionary, as not all pages are the same\n",
    "for i in a:\n",
    "    \n",
    "    # Unwanted characters to be removed\n",
    "    i['Outcome']=str(re.sub(\"\\\\\\\\\",\"\",i['Outcome']))\n",
    "    i['Outcome']=str(re.sub(\"\\.,\",\"\\. \",i['Outcome']))\n",
    "    i['Outcome']=str(re.sub(\"\\'\",\" \\' \",i['Outcome']))\n",
    "    i['Outcome']=str(re.sub(\"\\'\",\"\",str(i['Outcome'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Students who have completed this unit will have an enhanced ability to critically analyse and evaluate the concepts employed in, and the doctrines accepted by, the world  s religions, as well as to put forward ideas and arguments of their own in a clear and coherent way. (\"Monash Graduates will be critical and creative scholars who apply research skills to a range of challenges, and communicate perceptively and effectively.\") Students who have completed this unit will have an enhanced knowledge of theories of religion, and of key issues in contemporary studies of religion.  \n",
      "==================\n",
      "{'Unit': 'ATS1040', 'Outcome': '  Students who have completed this unit will have an enhanced ability to critically analyse and evaluate the concepts employed in, and the doctrines accepted by, the world  s religions, as well as to put forward ideas and arguments of their own in a clear and coherent way. (\"Monash Graduates will be critical and creative scholars who apply research skills to a range of challenges, and communicate perceptively and effectively.\") Students who have completed this unit will have an enhanced knowledge of theories of religion, and of key issues in contemporary studies of religion.  ', 'Synopsis': 'This unit is a general introduction to the study of religions, spirituality, and belief and non-belief, in the contemporary world. In particular, it introduces a range of theoretical and methodological approaches, and examines some of the pressing intellectual questions that are raised by (and for) religions, spirituality, belief and non-belief. '}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "# Sample outcome\n",
    "for i in a:\n",
    "    if i['Unit']==\"ATS1040\":\n",
    "        print(i['Outcome'])\n",
    "        print(\"==================\")\n",
    "        print(i)\n",
    "## We observe that 'ATS2172' unit is repeated 2 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit list containing the combined Synopsis and Outcome text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To combine outcome and synopsis to create a new list\n",
    "unit_list=list()\n",
    "\n",
    "for i in a:\n",
    "    unit_dict=dict()\n",
    "    unit=i[\"Unit\"]\n",
    "    x=re.sub(\"\\.\\'\",\"\\.\",i['Outcome']+\" \"+i['Synopsis'])\n",
    "    unit_dict[\"unit\"]=unit\n",
    "    \n",
    "    # Removing unwanted characters\n",
    "    unit_dict[\"text\"]=re.sub(\"\\\\\\\\\",\"\",str(x))\n",
    "    \n",
    "    unit_list.append(unit_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the text for all the units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all the units to tokens and calculate bigrams\n",
    "doc_dict=dict()\n",
    "j=\"\"\n",
    "for i in a:\n",
    "    x=re.sub(\"\\.\\'\",\"\\.\",i['Outcome']+\" \"+i['Synopsis'])\n",
    "    \n",
    "    # Refining the dictionary\n",
    "    j=j+re.sub(\"\\\\\\\\\",\"\",str(x))\n",
    "    j=re.sub(\"\\.\\ \\'\",\".\",str(j))\n",
    "    j=re.sub(\"\\.\\'\",\".\",str(j))\n",
    "    j=re.sub(\";\",\".\",str(j))\n",
    "    \n",
    "doc_dict['29389429']=j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the abstracts \n",
    "\n",
    "The goal of text pre-processing is to convert unstructure text into structured data. For this analysis, a sparse count vector is produced.\n",
    "\n",
    "Pre-processing of the abstract text is achieved using the following steps. Here we consider as the order to be very important:\n",
    "  -   Conversion to lower case for only the first element of the sentence and keeping the middle words capital as it is\n",
    "  -   Tokenising in order to obtain all the tokens with length greater than 3.\n",
    "  -   We choose to then find the top 200 bigrams because, if we remove stop words first, then we miss out on a few bigrams.\n",
    "  -   Retokenise for a particular unit in order to obtain bigrams.\n",
    "  -   Removal of stopwords context dependent and context independent\n",
    "  -   Porting and stemming the unique tokens obtained\n",
    "  -   Stemming the individual tokens in the unit in order to obtain the count\n",
    "\n",
    "### To obtain the count we stem the initial tokens. For example, if unit contains two words \"beautiful\" and \"beautifully\" then to obtain a count of 2 for the dictionary vocabulary word \"beauti\" we need to stem them as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normalise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to convert the first word of the sentence to lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_lower(line):\n",
    "    list1=line.split()\n",
    "    #print(list1)\n",
    "    \n",
    "    list1[0]=list1[0].lower()\n",
    "    \n",
    "    if not re.findall(\"\\w+\",str(list1[0])):\n",
    "        list1[1]=list1[1].lower()\n",
    "    \n",
    "    for i in list1:   \n",
    "        c= \" \".join(list1)\n",
    "        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to convert the paragraph to sentences and lower the first word of the sentence\n",
    "* The below code is referenced from Tutorial 4 (FIT5196- Data Wrangling, Monash University)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the paragraph\n",
    "import nltk.data\n",
    "def text_sent(paragraph):\n",
    "    \n",
    "    #\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    sentences = sent_detector.tokenize(paragraph)\n",
    "    j=\"\"\n",
    "    for sent in sentences:\n",
    "        first_lower(sent)\n",
    "        \n",
    "        j=j+\" \"+first_lower(sent)\n",
    "        \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalising for the entire document as well as for individual units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the entire dictionary\n",
    "new_doc_dict=dict()\n",
    "new_doc_dict['29389429']=text_sent(doc_dict['29389429'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the unit\n",
    "for unit in unit_list:\n",
    "    unit['text']=text_sent(unit['text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is reference from Tutorial 4 (FIT5196-Data Wrangling, Monash University S1 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "### Tokenising using the provided regex\n",
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "doc_uni_tokens = tokenizer.tokenize(new_doc_dict['29389429'])\n",
    "\n",
    "# Removing tokens with length less than 3\n",
    "uni_tokens=[w for w in doc_uni_tokens if len(w)>=3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Tokenising individual unit  \n",
    "Also, we remove token with length less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "\n",
    "for i in unit_list:\n",
    "    ## Removing tokens with len < 3\n",
    "    unigram_tokens = tokenizer.tokenize(i['text'])\n",
    "    i['uni_tokens']=[w for w in unigram_tokens if len(w)>=3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bigram tokens from the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is reference from Tutorial 4 (FIT5196-Data Wrangling, Monash University S1 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(uni_tokens)\n",
    "\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)# or w.lower() in ignored_words)\n",
    "\n",
    "# Finding the top 200 bigrams\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200)\n",
    "\n",
    "bigrams_set=list(set(top_200_bigrams))\n",
    "\n",
    "#print(bigrams_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Retokenise and then remove context independent stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is reference from Tutorial 5 (FIT5196-Data Wrangling, Monash University S1 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the stop words\n",
    "stopwords=list()\n",
    "\n",
    "\n",
    "with open(\"stopwords_en.txt\",\"r\") as words:\n",
    "    for i in words.readlines():\n",
    "        stopwords.append(i.strip('\\n'))\n",
    "# Set of unique stopwords\n",
    "stopwords_set = set(stopwords)\n",
    "\n",
    "# Retokenise to remove context independent stop words from the given list\n",
    "for i in unit_list:\n",
    "    from nltk.tokenize import MWETokenizer\n",
    "    # Tokenise bigrams\n",
    "    mwe_tokenizer = MWETokenizer(bigrams_set)\n",
    "    \n",
    "    # Retokenising them\n",
    "    tokens_sw = mwe_tokenizer.tokenize(i['uni_tokens'])\n",
    "    i['uni_tokens'] = [w for w in tokens_sw if w not in stopwords_set]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note\n",
    "- If we stem the words before removing the context independent words, a few words providing additional information about the unit would be captured.\n",
    "- However, if lets say words \"pollutants\",\"polluting\" and \"pollutant\" occur in 33% of the documents each exclusively. then if we stem them \"pollut\" word will occur in 99% of the documents and will be removed.\n",
    "- It is for the same reason that we do not stem before removal of context dependent words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a set of tokens with bigrams and unigrams unique to a unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating unit set of tokens to calculate the document frequency\n",
    "for i in unit_list:\n",
    "    i['re_set']=list(set(i['uni_tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13201"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set of unique tokens in a particular unit\n",
    "token_count=list()\n",
    "\n",
    "for i in unit_list:\n",
    "    \n",
    "    for j in i['re_set']:\n",
    "        \n",
    "        token_count.append(j)\n",
    "        \n",
    "len(token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['academically', 'graduate', 'results', 'skills', 'process', 'independently', 'literature', 'project', 'students', 'formulate', 'continue', 'demonstrate', 'command', 'conduct', 'written', 'research', 'independent', 'supervision', 'suitable', 'academic', 'thesis', 'production', 'critically', 'sophisticated', 'argument', 'sound', 'staff', 'develop', 'commenced', 'relevant', 'APG5849_Graduate', 'analyse']\n",
      "==================\n",
      "32\n",
      "==================\n",
      "['formulate', 'research', 'project', 'conduct', 'research', 'independently', 'demonstrate', 'command', 'relevant', 'literature', 'critically', 'analyse', 'relevant', 'academic', 'literature', 'develop', 'academically', 'sound', 'argument', 'demonstrate', 'sophisticated', 'written', 'skills', 'supervision', 'suitable', 'academic', 'staff', 'students', 'continue', 'conduct', 'independent', 'research', 'commenced', 'APG5849_Graduate', 'research', 'thesis', 'research', 'process', 'results', 'production', 'graduate', 'research', 'thesis']\n",
      "==================\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "# Sample output\n",
    "for i in unit_list:\n",
    "    \n",
    "    if i['unit']==\"APG5850\":\n",
    "        \n",
    "        print(i['re_set'])\n",
    "        print(\"==================\")\n",
    "        print(len(i['re_set'])) # Set of unique tokens\n",
    "        \n",
    "        print(\"==================\")\n",
    "        print(i['uni_tokens'])\n",
    "        print(\"==================\")\n",
    "        \n",
    "        print(len(i['uni_tokens'])) # List of tokens in the unit including repititions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Removing context dependent stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is reference from Exploring Pre-Processed text and Generating Features.ipynb (FIT5196-Data Wrangling, Monash University S1 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3566"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "from nltk.probability import *\n",
    "\n",
    "# It is the count of all unique tokens across the documents\n",
    "freqdict=FreqDist(token_count)\n",
    "tdict=dict(freqdict)\n",
    "\n",
    "# Unique count of vocabulary in the document with frequency\n",
    "len(tdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We see that the word unit appears more than 160 times\n",
    "freqdict.plot(25, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is reference from Exploring Pre-Processed text and Generating Features.ipynb (FIT5196-Data Wrangling, Monash University S1 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plotting the words with frequency of frequency as 1\n",
    "\n",
    "ffd = FreqDist(freqdict.values())\n",
    "\n",
    "from pylab import *\n",
    "y = [0]*14\n",
    "\n",
    "for k, v in ffd.items():\n",
    "     if k <= 10:\n",
    "        y[k-1] = v\n",
    "     elif k >10 and k <= 50:\n",
    "        y[10] =  y[10] + v\n",
    "     elif k >50 and k <= 100:\n",
    "        y[11] =  y[11] + v\n",
    "     elif k > 100 and k <= 200:\n",
    "        y[12] =  y[12] + v\n",
    "     else:\n",
    "        y[13] =  y[13] + v\n",
    "        \n",
    "x = range(1, 15) # generate integer from 1 to 14\n",
    "ytks =list(map(str, range(1, 11))) # covert a integer list to a string list\n",
    "ytks.append('10-50')\n",
    "ytks.append('51-100')\n",
    "ytks.append('101-200')\n",
    "ytks.append('>200')\n",
    "barh(x,y, align='center')\n",
    "yticks(x, ytks)\n",
    "xlabel('Frequency of Frequency')\n",
    "ylabel('Word Frequency')\n",
    "grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows that there are a lot of words with frequency in between 1-10. infact aroung 90% of the words have the frequency less than 10 and hence will constitute as rare tokens and will get removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing tokens with unit frequency more than 95% or less than 5%\n",
    "\n",
    "* 95% for context dependent stop words\n",
    "* 5% for rare tokens\n",
    "\n",
    "#### However, there is a problem with this case that, bigrams such as \"Nurse_practitioners\" which are common in the nursing units get removed as the frequency in this case is not more than 5%.\n",
    "#### We are losing out on important information pertaining to a particular unit.\n",
    "\n",
    "#### Ideally, if we are having bigrams then we should not filter them for a frequency of 5%. There should be a different threshold for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Token list after removing context dependent stop words\n",
    "new_token_list=list()\n",
    "\n",
    "for k,v in tdict.items():\n",
    "    # Frequency of 95% for context\n",
    "    if v>10 and v<=190:\n",
    "        new_token_list.append(k)\n",
    "        \n",
    "len(new_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the unit list\n",
    "unit_list objects contains:\n",
    "* uni_tokens: individual tokens in a single unit.\n",
    "We need to stem these tokens in order to find the frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['academically', 'graduate', 'results', 'skills', 'process', 'independently', 'literature', 'project', 'students', 'formulate', 'continue', 'demonstrate', 'command', 'conduct', 'written', 'research', 'independent', 'supervision', 'suitable', 'academic', 'thesis', 'production', 'critically', 'sophisticated', 'argument', 'sound', 'staff', 'develop', 'commenced', 'relevant', 'APG5849_Graduate', 'analyse']\n",
      "==================\n",
      "32\n",
      "==================\n",
      "['formulate', 'research', 'project', 'conduct', 'research', 'independently', 'demonstrate', 'command', 'relevant', 'literature', 'critically', 'analyse', 'relevant', 'academic', 'literature', 'develop', 'academically', 'sound', 'argument', 'demonstrate', 'sophisticated', 'written', 'skills', 'supervision', 'suitable', 'academic', 'staff', 'students', 'continue', 'conduct', 'independent', 'research', 'commenced', 'APG5849_Graduate', 'research', 'thesis', 'research', 'process', 'results', 'production', 'graduate', 'research', 'thesis']\n",
      "==================\n",
      "43\n",
      "==================\n",
      "['results', 'skills', 'process', 'literature', 'project', 'students', 'demonstrate', 'conduct', 'written', 'research', 'independent', 'academic', 'production', 'critically', 'develop', 'relevant', 'analyse', 'occupational', 'application', 'science', 'concept', 'conceptual', 'work', 'range', 'form', 'effectively', 'practice', 'part', 'techniques', 'nature', 'technical', 'projects', 'health', 'design', 'ideas', 'industry', 'ability', 'designs', 'communicate', 'understand', 'safety', 'unit', 'focus', 'understanding', 'contemporary', 'oral', 'reflect', 'learning', 'relating', 'Australia', 'common', 'system', 'examined', 'legal', 'presentation', 'international', 'perspectives', 'interpret', 'assess', 'role', 'identify', 'control', 'rules', 'issues', 'Australian', 'contexts', 'order', 'policy', 'writing', 'historical', 'practical', 'advanced', 'evaluate', 'applied', 'framework', 'present', 'principles', 'apply', 'professional', 'context', 'developed', 'theories', 'integrate', 'areas', 'explore', 'settings', 'processes', 'history', 'theory', 'clinical', 'address', 'communication', 'critique', 'explores', 'approaches', 'planning', 'systems', 'complex', 'problems', 'analytical', 'ethical', 'outcomes', 'study', 'capacity', 'high', 'level', 'tools', 'examine', 'strategies', 'frameworks', 'concepts', 'field', 'environmental', 'management', 'social', 'business', 'challenges', 'case', 'factors', 'aims', 'relationship', 'aspects', 'assessment', 'solutions', 'analysis', 'knowledge', 'describe', 'implementation', 'developing', 'expected', 'investigation', 'data', 'explain', 'report', 'comprehensive', 'software', 'methods', 'required', 'write', 'select', 'groups', 'methodology', 'critical', 'making', 'debates', 'informed', 'including', 'awareness', 'engage', 'aim', 'world', 'introduces', 'introduced', 'cultural', 'theoretical', 'basic', 'specific', 'complete', 'community', 'scientific', 'emphasis', 'studies', 'undertake', 'presentations', 'discuss', 'discipline', 'arguments', 'current', 'main', 'political', 'media', 'major', 'development', 'thinking', 'plan', 'building', 'team', 'life', 'appraise', 'variety', 'sources', 'findings', 'examines', 'fundamental', 'reference', 'opportunity', 'change', 'individual', 'public', 'activities', 'produce', 'chosen', 'questions', 'global', 'approach', 'area', 'topics', 'environments', 'organisational', 'covered', 'solving', 'implications', 'structure', 'performance', 'group', 'problem', 'include', 'based', 'society', 'culture', 'care', 'texts', 'environment', 'information', 'requirements', 'disease', 'provide', 'existing', 'relation', 'related', 'roles', 'recognise', 'series', 'practices', 'material', 'language', 'applications', 'introduction', 'construct', 'influence', 'key', 'program', 'topic', 'designed', 'digital', 'relationships', 'economic', 'terms', 'working', 'impact', 'setting', 'effective', 'models', 'evidence', 'personal', 'strengths', 'importance', 'ethics', 'implement', 'manage', 'resources']\n"
     ]
    }
   ],
   "source": [
    "# Sample output\n",
    "for i in unit_list:\n",
    "    \n",
    "    if i['unit']==\"APG5850\":\n",
    "        \n",
    "        print(i['re_set'])\n",
    "        print(\"==================\")\n",
    "        \n",
    "        print(len(i['re_set'])) # Set of unique tokens\n",
    "        print(\"==================\")\n",
    "        \n",
    "        print(i['uni_tokens'])\n",
    "        print(\"==================\")\n",
    "        \n",
    "        print(len(i['uni_tokens'])) # List of tokens in the unit including repititions\n",
    "\n",
    "print(\"==================\")\n",
    "print(new_token_list) # contains no bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using Porter Stemmer to stem the unigrams\n",
    "\n",
    "We stem only the unigrams as stemming the bigrams is not useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "final_tokens_1 =[]\n",
    "# We make use of the unique token list \n",
    "for token in new_token_list:\n",
    "    if \"_\" not in token:\n",
    "        final_tokens_1 = final_tokens_1 + [stemmer.stem(token)]\n",
    "    else:\n",
    "        final_tokens_1=final_tokens+[token]\n",
    "\n",
    "# These are the set of Unique tokens left after aggregating the porter stemmed tokens\n",
    "final_tokens_1=list(set(final_tokens_1))\n",
    "\n",
    "len(final_tokens_1) # 200 unique tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We stem the bigrams in particular unit in order to obtain the count from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We stem the unigrams in the list of tokens inside each unit\n",
    "# Note: retokenised key contains list of all tokens inside one unit including unigram and bigrams\n",
    "\n",
    "for i in unit_list:\n",
    "    final_tokens=[]\n",
    "    \n",
    "    for token in i['uni_tokens']:\n",
    "        if \"_\" not in token:\n",
    "            final_tokens = final_tokens + [stemmer.stem(token)]\n",
    "            \n",
    "        # We do not want to stem the bigrams    \n",
    "        else:\n",
    "            final_tokens=final_tokens+[token]\n",
    "    \n",
    "    # Final tokens in unit list contains the tokens that need to be counted\n",
    "    i['final_tokens']=final_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a. Creating the dictionary of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('abil', 1), ('academ', 2), ('activ', 3), ('address', 4), ('advanc', 5), ('aim', 6), ('analys', 7), ('analysi', 8), ('analyt', 9), ('appli', 10), ('applic', 11), ('apprais', 12), ('approach', 13), ('area', 14), ('argument', 15), ('aspect', 16), ('assess', 17), ('australia', 18), ('australian', 19), ('awar', 20), ('base', 21), ('basic', 22), ('build', 23), ('busi', 24), ('capac', 25), ('care', 26), ('case', 27), ('challeng', 28), ('chang', 29), ('chosen', 30), ('clinic', 31), ('common', 32), ('commun', 33), ('complet', 34), ('complex', 35), ('comprehens', 36), ('concept', 37), ('conceptu', 38), ('conduct', 39), ('construct', 40), ('contemporari', 41), ('context', 42), ('control', 43), ('cover', 44), ('critic', 45), ('critiqu', 46), ('cultur', 47), ('current', 48), ('data', 49), ('debat', 50), ('demonstr', 51), ('describ', 52), ('design', 53), ('develop', 54), ('digit', 55), ('disciplin', 56), ('discuss', 57), ('diseas', 58), ('econom', 59), ('effect', 60), ('emphasi', 61), ('engag', 62), ('environ', 63), ('environment', 64), ('ethic', 65), ('evalu', 66), ('evid', 67), ('examin', 68), ('exist', 69), ('expect', 70), ('explain', 71), ('explor', 72), ('factor', 73), ('field', 74), ('find', 75), ('focu', 76), ('form', 77), ('framework', 78), ('fundament', 79), ('global', 80), ('group', 81), ('health', 82), ('high', 83), ('histor', 84), ('histori', 85), ('idea', 86), ('identifi', 87), ('impact', 88), ('implement', 89), ('implic', 90), ('import', 91), ('includ', 92), ('independ', 93), ('individu', 94), ('industri', 95), ('influenc', 96), ('inform', 97), ('integr', 98), ('intern', 99), ('interpret', 100), ('introduc', 101), ('introduct', 102), ('investig', 103), ('issu', 104), ('key', 105), ('knowledg', 106), ('languag', 107), ('learn', 108), ('legal', 109), ('level', 110), ('life', 111), ('literatur', 112), ('main', 113), ('major', 114), ('make', 115), ('manag', 116), ('materi', 117), ('media', 118), ('method', 119), ('methodolog', 120), ('model', 121), ('natur', 122), ('occup', 123), ('opportun', 124), ('oral', 125), ('order', 126), ('organis', 127), ('outcom', 128), ('part', 129), ('perform', 130), ('person', 131), ('perspect', 132), ('plan', 133), ('polici', 134), ('polit', 135), ('practic', 136), ('present', 137), ('principl', 138), ('problem', 139), ('process', 140), ('produc', 141), ('product', 142), ('profession', 143), ('program', 144), ('project', 145), ('provid', 146), ('public', 147), ('question', 148), ('rang', 149), ('recognis', 150), ('refer', 151), ('reflect', 152), ('relat', 153), ('relationship', 154), ('relev', 155), ('report', 156), ('requir', 157), ('research', 158), ('resourc', 159), ('result', 160), ('role', 161), ('rule', 162), ('safeti', 163), ('scienc', 164), ('scientif', 165), ('select', 166), ('seri', 167), ('set', 168), ('skill', 169), ('social', 170), ('societi', 171), ('softwar', 172), ('solut', 173), ('solv', 174), ('sourc', 175), ('specif', 176), ('strategi', 177), ('strength', 178), ('structur', 179), ('student', 180), ('studi', 181), ('system', 182), ('team', 183), ('technic', 184), ('techniqu', 185), ('term', 186), ('text', 187), ('theoret', 188), ('theori', 189), ('think', 190), ('tool', 191), ('topic', 192), ('understand', 193), ('undertak', 194), ('unit', 195), ('varieti', 196), ('work', 197), ('world', 198), ('write', 199), ('written', 200)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count=1\n",
    "diction_out=list()\n",
    "final_tokens_1.sort()\n",
    "\n",
    "for i in final_tokens_1:\n",
    "    diction_out.append((i,count))\n",
    "    count+=1\n",
    "    \n",
    "print(diction_out)\n",
    "\n",
    "\n",
    "# Writing the dictionary\n",
    "with open(\"29389429_vocab.txt\", \"w\") as diction_file:\n",
    "    \n",
    "    for i in diction_out:\n",
    "        \n",
    "        diction_file.write(str(i[0])+\":\"+str(i[1])+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b. Creating the output vector space model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list=list()\n",
    "for j in unit_list:\n",
    "    \n",
    "    # Creating a list of lists to iterate through in order to write\n",
    "    a_list=list()\n",
    "    blist=list()\n",
    "\n",
    "    for i in diction_out:\n",
    "    \n",
    "        \n",
    "        if j['final_tokens'].count(i[0])!=0:\n",
    "            blist.append((i[1],j['final_tokens'].count(i[0])))\n",
    "\n",
    "\n",
    "    # Inorder to check the print output\n",
    "    #print(j['unit']) \n",
    "    #print(count)\n",
    "    #print(\"===================\")\n",
    "    #print(blist)\n",
    "    #print(\"===================\")\n",
    "    \n",
    "    a_list.append(j['unit'])\n",
    "    a_list.append(blist)\n",
    "    c_list.append(a_list)\n",
    "    \n",
    "\n",
    "# Writing the vector\n",
    "with open(\"Output_countVec.txt\", \"w\") as a_file:\n",
    "    for i in c_list:\n",
    "        a_file.write(i[0]+\",\")\n",
    "        #print(i)\n",
    "\n",
    "        count=0\n",
    "        for j in i[1]:\n",
    "          #  print(j)\n",
    "            count+=1\n",
    "            \n",
    "            # We do not append the comma at the end\n",
    "            if count==len(i[1]):\n",
    "                    a_file.write(str(j[0])+\":\"+str(j[1]))\n",
    "            else:\n",
    "\n",
    "                a_file.write(str(j[0])+\":\"+str(j[1])+\",\")\n",
    "        a_file.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
